confusionMatrix(yhat.ran.multi,ran.test.multi)
varImpPlot(ran.multi)
ran.bi <- randomForest(Genotype~., data=df2_train, mtry=9,
importance =TRUE)
yhat.ran.bi <- predict(ran.bi ,newdata = df2_test)
table(predict=yhat.ran.bi, truth=df2_test$Genotype)
#173+152 = 325/330 = 0.98
confusionMatrix(yhat.ran.bi,df2_test$Genotype)
varImpPlot(ran.bi)
varImpPlot(bag)
library(MASS)
library(ISLR)
library(readxl)
library(dplyr)
library(tree)
library(e1071)
setwd("~/Master Program/Statistical Learning/Assignments/HomeAssignment 2")
# Reading excel file as dtaframe
df <-read_excel("Data_Cortex_Nuclear.xls")
# Removing the index
df <- df[,c(2:82)]
# Replacing null values
for(i in 1:77){
df[is.na(df[,i]), i] <- min(df[,i], na.rm = TRUE)
}
# Determine number of clusters
wss <- (nrow(df[,-c(78:81)])-1)*sum(apply(df[,-c(78:81)],2,var))
for (i in 1:15) wss[i] <- sum(kmeans(df[,-c(78:81)],
centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
data_Cluster <- kmeans(df[,-c(78:81)], 2, nstart = 20)
data_Cluster
cluster_data<-df
cluster_data<-cbind(cluster_data,data_Cluster$cluster)
table(data_Cluster$cluster, cluster_data$Treatment)
table(data_Cluster$cluster, cluster_data$Behavior)
table(data_Cluster$cluster, cluster_data$Genotype)
# Ward Hierarchical Clustering
d <- dist(df[,-c(78:81)], method = "euclidean") # distance matrix
fit <- hclust(d, method="ward.D2")
plot(fit) # display dendogram
# draw dendogram with red borders around the 8 clusters
rect.hclust(fit, k=8, border="red")
groups <- cutree(fit, k=8) # cut tree into 8 clusters
groups
cluster_data<-df
cluster_data<-cbind(cluster_data,groups)
table(cluster_data$groups, cluster_data$Genotype)
# Ward Hierarchical Clustering
d <- dist(df[,-c(78:81)], method = "euclidean") # distance matrix
fit <- hclust(d, method="ward.D2")
plot(fit) # display dendogram
# draw dendogram with red borders around the 8 clusters
rect.hclust(fit, k=8, border="red")
# draw dendogram with red borders around the 8 clusters
rect.hclust(fit, k=5, border="red")
# Ward Hierarchical Clustering
d <- dist(df[,-c(78:81)], method = "euclidean") # distance matrix
fit <- hclust(d, method="ward.D2")
plot(fit) # display dendogram
# draw dendogram with red borders around the 8 clusters
rect.hclust(fit, k=5, border="red")
# draw dendogram with red borders around the 8 clusters
rect.hclust(fit, k=8, border="red")
# Ward Hierarchical Clustering
d <- dist(df[,-c(78:81)], method = "euclidean") # distance matrix
fit <- hclust(d, method="ward.D2")
plot(fit) # display dendogram
# draw dendogram with red borders around the 8 clusters
rect.hclust(fit, k=8, border="red")
df1$class <- as.factor(df1$class)
df2$Genotype <- as.factor(df2$Genotype)
rm(list = ls())
rm(list = ls())
library(MASS)
library(ISLR)
library(readxl)
library(dplyr)
library(tree)
library(e1071)
library(vctrs)
library(tibble)
library(VIM)
library(tidyverse)
library(caret)
setwd("~/Master Program/Statistical Learning/Assignments/HomeAssignment 2")
# Reading excel file as dtaframe
df <-read_excel("Data_Cortex_Nuclear.xls")
# Removing the index
df <- df[,c(2:82)]
# Replacing null values
for(i in 1:77){
df[is.na(df[,i]), i] <- min(df[,i], na.rm = TRUE)
}
#Check whether nullvalues removed or not
sum(is.na(df))
# Multi-class Dataset
df1 <- df[,c(1:77,81)]
# Binary-class Dataset
df2 <- df[,c(1:78)]
set.seed(201345)
df1$class <- as.factor(df1$class)
df2$Genotype <- as.factor(df2$Genotype)
# Sampling the dataset
train <- sample(1:nrow(df),750)
# Multi-class Train and Test Dataset
df1_train <- df1[train,]
df1_test <- df1[-train,]
# Binary-class Train and Test Dataset
df2_train <- df2[train,]
df2_test <- df2[-train,]
################ Multi-class: Decision Tree #####################
tree.df1_train <- tree(class ~., df1_train)
summary(tree.df1_train)
yhat.bag <- predict(tree.df1_train ,df1_test,type="class")
table(yhat.bag,df1_tests)
table(yhat.bag,df1_test)
table(yhat.bag,df1_test$class)
#table(yhat.bag,df1_test$class)
confusionMatrix(yhat.bag,df1_tests)
#table(yhat.bag,df1_test$class)
confusionMatrix(yhat.bag,df1_test)
#table(yhat.bag,df1_test$class)
confusionMatrix(yhat.bag,df1_test$class)
################ Binary-class: Decision Tree #####################
tree.df2_train <- tree(df2_train$Genotype ~., df2_train)
summary(tree.df2_train)
yhat.bag2 <- predict(tree.df2_train ,df2_test, type="class")
table(yhat.bag2,df2_test$Genotype)
confusionMatrix(yhat.bag2,df2_test$Genotype)
####################################################
#Use cross-validation in order to determine the optimal level of tree complexity.
#Does pruning the tree improve the test MSE?
################ Multi-class: Decision Tree #####################
set.seed(367634)
cv.df1_train <-cv.tree(tree.df1_train, FUN=prune.misclass)
names(cv.df1_train)
cv.df1_train
par(mfrow =c(1,2))
plot(cv.df1_train$size ,cv.df1_train$dev ,type="b")
plot(cv.df1_train$k ,cv.df1_train$dev ,type="b")
prune.df1_train <- prune.misclass(tree.df1_train , best =20)
pred.test3 <- predict(prune.df1_train ,df1_test,type="class")
table(pred.test3,df1_test$class)
confusionMatrix(pred.test3,df1_test$class)
#42+23+40+28+19+25+38+31 = 246/330 = 0.745
################ Binary-class: Decision Tree #####################
set.seed(367634)
cv.df2_train <-cv.tree(tree.df2_train ,FUN=prune.misclass)
names(cv.df2_train)
cv.df2_train
par(mfrow =c(1,2))
plot(cv.df2_train$size ,cv.df2_train$dev ,type="b")
plot(cv.df2_train$k ,cv.df2_train$dev ,type="b")
prune.df2_train <- prune.misclass(tree.df2_train , best =18)
pred.test4 <- predict(prune.df2_train ,df2_test,type="class")
table(pred.test4,df2_test$Genotype)
#148+132 = 280/329 = 0.85
confusionMatrix(pred.test4,df2_test$Genotype)
########## Multi-class:support vector classifier-Linear ################
## Linear
svmfit_linear <- svm(class~., data=df1_train, kernel ="linear", cost = 0.1,
scale =FALSE)
ypred <- predict(svmfit_linear ,df1_test)
table(predict = ypred , truth= df1_test$class)
#24+21+50+34+36+27+26+35 = 253/330= 0.76
confusionMatrix(ypred,df1_test$class)
## Linear
svmfit_linear <- svm(class~., data=df1_train, kernel ="linear", cost = 0.01,
scale =FALSE)
ypred <- predict(svmfit_linear ,df1_test)
table(predict = ypred , truth= df1_test$class)
#19+9+48+30+18+1+10+27 =  162/330 = 0.49
confusionMatrix(ypred,df1_test$class)
#  Fit a support vector classifier to the data with various values of cost, Report the
# cross-validation errors associated with different values of this parameter
set.seed(24583)
tune.out1 <- tune(svm, class ~., data=df1_train, kernel = "linear",
ranges = list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out1)
bestmod1 <- tune.out1$best.model
#Number of Support Vectors:  327
#Best model 1 & Applying it
svmfit_linear <- svm(class~., data=df1_train, kernel ="linear", cost = 1,
scale =FALSE)
ypred <- predict(svmfit_linear ,df1_test)
table(predict = ypred , truth= df1_test$class)
#46+27+50+35+36+32+42+35 = 303/330= 0.91
confusionMatrix(ypred,df1_test$class)
############### Multi-class:support vector classifier-Radial #####################
svmfit_RBF1 <- svm(class~., data=df1_train, kernel ="radial", gamma = 0.1,
cost = 0.1)
ypredr1 <- predict(svmfit_RBF1 ,df1_test)
table(predict = ypredr1 , truth=df1_test$class)
#50+0+43+31+0+0+0+35 = 159/330 = 0.48
confusionMatrix(ypredr1,df1_test$class)
#################
set.seed(142342)
tune.outr1 <- tune(svm, class ~., data=df1_train, kernel = "radial",
ranges = list(cost=c(0.001, 0.01, 0.1, 1,5,10,100),
gamma= c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.outr1)
bestmodr1 <- tune.outr1$best.model
svmfit_RBF1 <- svm(class~., data=df1_train, kernel ="radial", gamma = 0.001,
cost = 100)
ypredr1 <- predict(svmfit_RBF1 ,df1_test)
table(predict = ypredr1 , truth=df1_test$class)
#50+38+50+34+44+33+45+35 = 329/330 = 0.996
confusionMatrix(ypredr1,df1_test$class)
svmfit_poly1 <- svm(class~., data=df1_train, kernel="polynomial",degree = 2, gamma = 0.1,
cost = 0.01)
ypredp1 <- predict(svmfit_poly1 ,df1_test)
table(predict = ypredp1 , truth=df1_test$class)
#50+37+50+34+40+31+39+32 = 313/330 = 0.948
confusionMatrix(ypredp1,df1_test$class)
#Tune
set.seed(142342)
tune.outp1 <- tune(svm, class ~., data=df1_train, kernel = "polynomial",
ranges = list(cost=c(0.001, 0.01, 0.1, 1,5,10,100),
degree=c(2,3,4)))
summary(tune.outp1)
bestmodp1 <- tune.outp1$best.model
#Number of Support Vectors:  404
svmfit_poly2 <- svm(class~., data=df1_train, kernel="polynomial",degree = 3,
cost = 5)
ypredp2 <- predict(svmfit_poly2 ,df1_test)
table(predict = ypredp2 , truth=df1_test$class)
#50+38+50+35+44+33+45+35 = 330/330 = 100
confusionMatrix(ypredp2,df1_test$class)
svmfit_linearbl1 <- svm(Genotype~., data = df2_train, kernel ="linear", cost = 0.01,
scale =FALSE)
ypredbl1 <- predict(svmfit_linearbl1 ,df2_test)
table(predict = ypredbl1 , truth= df2_test$Genotype)
#141+62 = 203/330 = 0.615
confusionMatrix(ypredbl1,df2_test$Genotype)
##Tune
set.seed(142342)
tune.outbl1 <- tune(svm, Genotype ~., data=df2_train, kernel = "linear",
ranges = list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.outbl1)
bestmod1 <- tune.outbl1$best.model
#Number of Support Vectors:  152
svmfit_linearbl1 <- svm(Genotype~., data = df2_train, kernel ="linear",
cost = 0.1, scale =FALSE)
ypredbl1 <- predict(svmfit_linearbl1 ,df2_test)
table(predict = ypredbl1 , truth= df2_test$Genotype)
confusionMatrix(ypredbl1,df2_test$Genotype)
cluster_data
data_Cluster$cluster
rm(list = ls())
library(MASS)
library(ISLR)
library(readxl)
library(dplyr)
library(tree)
library(e1071)
setwd("~/Master Program/Statistical Learning/Assignments/HomeAssignment 2")
# Reading excel file as dtaframe
df <-read_excel("Data_Cortex_Nuclear.xls")
# Removing the index
df <- df[,c(2:82)]
# Replacing null values
for(i in 1:77){
df[is.na(df[,i]), i] <- min(df[,i], na.rm = TRUE)
}
# Determine number of clusters
wss <- (nrow(df[,-c(78:81)])-1)*sum(apply(df[,-c(78:81)],2,var))
for (i in 1:15) wss[i] <- sum(kmeans(df[,-c(78:81)],
centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
data_Cluster <- kmeans(df[,-c(78:81)], 2, nstart = 20)
data_Cluster
cluster_data<-df
cluster_data<-cbind(cluster_data,data_Cluster$cluster)
cluster_data
table(data_Cluster$cluster, cluster_data$Treatment)
table(data_Cluster$cluster, cluster_data$Behavior)
table(data_Cluster$cluster, cluster_data$Genotype)
c1 <- subset(cluster_data$cluster==1)
cluster_data
c1 <- subset(cluster_data,cluster==1)
rm(list = ls())
rm(list = ls())
dev.off()
loadPackages <- function(package0){
package1 <- package0[!(package0 %in% installed.packages()[, "Package"])]
if (length(package1))
install.packages(package1, dependencies = TRUE)
sapply(package0, require, character.only = TRUE)
}
Packages <- c("GGally","car", "MASS","gplots","rcompanion","outliers",
"tseries",
"corrplot", "RColorBrewer",
"dplyr","ggplot2")
loadPackages(Packages)
setwd("~/Master Program/Statistical Learning/Assignments/HomeAssignment 2")
# Reading excel file as dtaframe
rat_data <-read_excel("Data_Cortex_Nuclear.xls")
rat_data2<- read_excel("Data_Cortex_Nuclear.xls")
summary(rat_data2)
sum(is.na(rat_data))
for(i in 2:78){
d<-rat_data[i]
d<-na.omit(d)
if(sum(is.na(rat_data[i]))>0){
rat_data[i][is.na(rat_data[i])] <- sum(d)/nrow(d)
}
}
for(i in 2:78){
d<-rat_data[i]
d<-na.omit(d)
if(sum(is.na(rat_data2[i]))>0){
rat_data2[i][is.na(rat_data2[i])] <- sum(d)/nrow(d)
}
}
X = rat_data[2:78]
cc<-scale(X)
library(cluster)
clusplot(X,
y_hc,
lines = 0,
shade = TRUE,
color = TRUE,
labels= 2,
plotchar = FALSE,
span = TRUE)
###############kmeans
library(factoextra)
cc<-scale(X)
distance <- get_dist(cc)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
k2 <- kmeans(cc, centers = 2, nstart = 25)
str(k2)
fviz_cluster(k2, data = cc)
cluster_data$cluster<-k2$cluster
rat_data$cluster<-k2$cluster
View(rat_data)
Qq1 <- subset(cluster_data,cluster==1)
Qq1 <- subset(rat_data,cluster==1)
Qq2 <- subset(rat_data,cluster==2)
Qq3 <- subset(rat_data,cluster==3)
Qq4 <- subset(rat_data,cluster==4)
Qq5<- subset(rat_data,cluster==5)
Qq6 <- subset(rat_data,cluster==6)
Qq7 <- subset(rat_data,cluster==7)
Qq8 <- subset(rat_data,cluster==8)
par(mfrow=c(3,3))
plot(Qq1$class,main="K-means cluster-1")
sum(is.na(Qq1))
Qq1$class
plot(Qq1$class)
Qq1$class
plot(as.factor((Qq1$class),main="K-means cluster-1")
plot(as.factor((Qq1$class)),main="K-means cluster-1")
plot(as.factor((Qq1$class)),main="K-means cluster-1")
plot(as.factor(Qq1$class),main="K-means cluster-1")
plot(as.factor(Qq2$class),main="K-means cluster-2")
plot(as.factor(Qq3$class),main="K-means cluster-3")
plot(as.factor(Qq3$class),main="K-means cluster-3")
plot(as.factor(Qq3$class))
plot(as.factor(Qq3$class),main="K-means cluster-3")
rm(list = ls())
dev.off()
loadPackages <- function(package0){
package1 <- package0[!(package0 %in% installed.packages()[, "Package"])]
if (length(package1))
install.packages(package1, dependencies = TRUE)
sapply(package0, require, character.only = TRUE)
}
Packages <- c("GGally","car", "MASS","gplots","rcompanion","outliers",
"tseries",
"corrplot", "RColorBrewer",
"dplyr","ggplot2")
loadPackages(Packages)
setwd("~/Master Program/Statistical Learning/Assignments/HomeAssignment 2")
# Reading excel file as dtaframe
rat_data <-read_excel("Data_Cortex_Nuclear.xls")
rat_data2<- read_excel("Data_Cortex_Nuclear.xls")
rat_data$class<- as.factor(rat_data$class)
for(i in 2:78){
d<-rat_data[i]
d<-na.omit(d)
if(sum(is.na(rat_data[i]))>0){
rat_data[i][is.na(rat_data[i])] <- sum(d)/nrow(d)
}
}
X = rat_data[2:78]
cc<-scale(X)
dendrogram = hclust(d = dist(cc, method = 'euclidean'), method = 'ward.D')
plot(dendrogram)
###############kmeans
library(factoextra)
cc<-scale(X)
distance <- get_dist(cc)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
k2 <- kmeans(cc, centers = 2, nstart = 25)
str(k2)
fviz_cluster(k2, data = cc)
x
rat_data$cluster<-k2$cluster
View(rat_data)
Qq1 <- subset(rat_data,cluster==1)
Qq2 <- subset(rat_data,cluster==2)
Qq3 <- subset(rat_data,cluster==3)
Qq4 <- subset(rat_data,cluster==4)
Qq5<- subset(rat_data,cluster==5)
Qq6 <- subset(rat_data,cluster==6)
Qq7 <- subset(rat_data,cluster==7)
Qq8 <- subset(rat_data,cluster==8)
plot(Qq4$class,main="K-means cluster-4")
plot(Qq5$class,main="K-means cluster-5")
plot(Qq6$class,main="K-means cluster-6")
plot(Qq7$class,main="K-means cluster-7")
plot(Qq8$class,main="K-means cluster-8")
par(mfrow=c(3,3))
plot(Qq4$class,main="K-means cluster-4")
par(mfrow=c(3,3))
plot(Qq4$class,main="K-means cluster-4")
par(mfrow=c(3,3))
plot(as.factor(Qq1$class),main="K-means cluster-1")
plot(as.factor(Qq2$class),main="K-means cluster-2")
plot(as.factor(Qq3$class),main="K-means cluster-3")
plot(Qq4$class,main="K-means cluster-4")
plot(Qq5$class,main="K-means cluster-5")
plot(Qq6$class,main="K-means cluster-6")
plot(Qq7$class,main="K-means cluster-7")
plot(Qq8$class,main="K-means cluster-8")
k2 <- kmeans(cc, centers = 8, nstart = 25)
str(k2)
rat_data$cluster<-k2$cluster
View(rat_data)
Qq1 <- subset(rat_data,cluster==1)
Qq2 <- subset(rat_data,cluster==2)
Qq3 <- subset(rat_data,cluster==3)
Qq4 <- subset(rat_data,cluster==4)
Qq5<- subset(rat_data,cluster==5)
Qq6 <- subset(rat_data,cluster==6)
Qq7 <- subset(rat_data,cluster==7)
Qq8 <- subset(rat_data,cluster==8)
par(mfrow=c(3,3))
plot(as.factor(Qq1$class),main="K-means cluster-1")
plot(as.factor(Qq2$class),main="K-means cluster-2")
plot(as.factor(Qq3$class),main="K-means cluster-3")
plot(Qq4$class,main="K-means cluster-4")
plot(Qq5$class,main="K-means cluster-5")
plot(Qq6$class,main="K-means cluster-6")
plot(Qq7$class,main="K-means cluster-7")
plot(Qq8$class,main="K-means cluster-8")
table(rat_data$cluster, rat_data$class)
#####2
cc<-scale(X)
X = rat_data[2:78]
cc<-scale(X)
distance <- get_dist(cc)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
rm(list = ls())
dev.off()
loadPackages <- function(package0){
package1 <- package0[!(package0 %in% installed.packages()[, "Package"])]
if (length(package1))
install.packages(package1, dependencies = TRUE)
sapply(package0, require, character.only = TRUE)
}
Packages <- c("GGally","car", "MASS","gplots","rcompanion","outliers",
"tseries",
"corrplot", "RColorBrewer",
"dplyr","ggplot2")
loadPackages(Packages)
setwd("~/Master Program/Statistical Learning/Assignments/HomeAssignment 2")
# Reading excel file as dtaframe
rat_data <-read_excel("Data_Cortex_Nuclear.xls")
rat_data$class<- as.factor(rat_data$class)
rat_data$Genotype <- as.factor(rat_data$Genotype)
for(i in 2:78){
d<-rat_data[i]
d<-na.omit(d)
if(sum(is.na(rat_data[i]))>0){
rat_data[i][is.na(rat_data[i])] <- sum(d)/nrow(d)
}
}
X = rat_data[2:78]
cc<-scale(X)
#####2
Y <- rat_data[2:78]
cc<-scale(Y)
distance <- get_dist(cc)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
k2 <- kmeans(cc, centers = 2, nstart = 25)
fviz_cluster(k2, data = cc)
rat_data$cluster<-k2$cluster
table(rat_data$cluster, rat_data$Genotype)
Qq1 <- subset(rat_data,cluster==1)
Qq2 <- subset(rat_data,cluster==2)
sum(is.na(Qq1))
par(mfrow=c(3,3))
plot(as.factor(Qq2$Genotype),main="K-means cluster-2")
par(mfrow=c(3,3))
plot(as.factor(Qq1$Genotype),main="K-means cluster-1")
plot(as.factor(Qq2$Genotype),main="K-means cluster-2")
table(rat_data$cluster, rat_data$Behavior)
Qq1 <- subset(rat_data,cluster==1)
Qq2 <- subset(rat_data,cluster==2)
sum(is.na(Qq1))
par(mfrow=c(3,3))
plot(as.factor(Qq1$Genotype),main="K-means cluster-1")
plot(as.factor(Qq2$Genotype),main="K-means cluster-2")
par(mfrow=c(3,3))
plot(as.factor(Qq1$Genotype),main="K-means cluster-1")
plot(as.factor(Qq2$Genotype),main="K-means cluster-2")
table(rat_data$cluster, rat_data$Treatment)
